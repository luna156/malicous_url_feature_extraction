{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0eP/v/Gq8r2DC6lsr+8mX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8fzURhyb8m5o"},"outputs":[],"source":["# Feature Engineering Refactoring\n","# 일괄 처리를 위한 함수화\n","def feature_extract(urldata):\n","# Length Feature\n","  #Length of URL\n","  urldata['url_length'] = urldata['url'].apply(lambda i: len(str(i)))\n","\n","  #Hostname Length\n","  urldata['hostname_length'] = urldata['url'].apply(lambda i: len(urlparse(i).netloc))\n","\n","  #Path Length\n","  urldata['path_length'] = urldata['url'].apply(lambda i: len(urlparse(i).path))\n","\n","  #First Directory Length\n","  def fd_length(url):\n","    urlpath= urlparse(url).path\n","    try:\n","        return len(urlpath.split('/')[1])\n","    except:\n","        return 0\n","  urldata['fd_length'] = urldata['url'].apply(lambda i: fd_length(i))\n","\n","  #Length of Top Level Domain\n","  urldata['tld'] = urldata['url'].apply(lambda i: get_tld(i,fail_silently=True))\n","  def tld_length(tld):\n","    try:\n","        return len(tld)\n","    except:\n","        return -1\n","  urldata['tld_length'] = urldata['tld'].apply(lambda i: tld_length(i))\n","  urldata = urldata.drop('tld', axis = 1)\n","#Count Feature\n","  #특수문자\n","  special_symbols = ['-', '@', '?', '%', '.', '=', 'http', 'https', 'www', '/', '//']\n","  for letter in special_symbols:\n","    urldata['count '+letter] = urldata['url'].apply(lambda i: i.count(letter))\n","\n","  #숫자\n","  def digit_count(url):\n","    digits = 0\n","    for i in url:\n","        if i.isnumeric():\n","            digits = digits + 1\n","    return digits\n","  urldata['count-digits']= urldata['url'].apply(lambda i: digit_count(i))\n","\n","  #알파벳\n","  def letter_count(url):\n","    letters = 0\n","    for i in url:\n","        if i.isalpha():\n","            letters = letters + 1\n","    return letters\n","  urldata['count-letters']= urldata['url'].apply(lambda i: letter_count(i))\n","\n","  # path부분의 /\n","  def no_of_dir(url):\n","    urldir = urlparse(url).path\n","    return urldir.count('/')\n","  urldata['count_dir'] = urldata['url'].apply(lambda i: no_of_dir(i))\n","\n","#Use of IP or not in domain\n","  def having_ip_address(url):\n","    match = re.search(\n","        '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n","        '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  # IPv4\n","        '((0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\/)' # IPv4 in hexadecimal\n","        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}', url)  # Ipv6\n","    if match: # 존재\n","        # print match.group()\n","        return 1\n","    else: # 없음\n","        # print 'No matching pattern found'\n","        return -1\n","  urldata['use_of_ip'] = urldata['url'].apply(lambda i: having_ip_address(i))\n","\n","#short link\n","  def shortening_service(url):\n","    match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n","                      'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n","                      'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n","                      'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n","                      'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n","                      'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n","                      'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|'\n","                      'tr\\.im|link\\.zip\\.net',\n","                      url)\n","    if match: # 존재\n","        return 1\n","    else: # 없음\n","        return -1\n","  urldata['short_url'] = urldata['url'].apply(lambda i: shortening_service(i))\n","\n","#URL 속에 파일 확장자가 들어있는가?\n","#파일 확장자가 들어있으면 1 , 없으면 -1\n","  def url_has_file(url):\n","    match = re.search('\\.exe|\\.zip|\\.reg|\\.rar|\\.js|\\.java|\\.lib|\\.log|\\.bat|\\.cmd|\\.vbs|\\.lnk|\\.php|\\.html|\\.htm|\\.hwp|\\.hwpx|\\.pptx|\\.docx|\\.iso|\\.xls|\\.xlsx',url)\n","    if match:\n","      return 1\n","    else:\n","      return -1\n","  urldata['url_has_file'] = urldata['url'].apply(lambda i: url_has_file(i))\n","\n","# URL 속에 Email 주소가 들어있는가?\n","# 있으면 1, 없으면 -1\n","  def url_has_email(url):\n","    match = re.search('\\w+\\@\\w+\\.\\w+' , url)\n","    if match:\n","      return 1\n","    else:\n","      return -1\n","  urldata['url_has_email'] = urldata['url'].apply(lambda i: url_has_email(i))\n","\n","#도메인과 URL의 길이 비율\n","  def len_Domain_ratio(url):\n","    url = urlparse(url)\n","    url_len = len(url)\n","    domain = len(url.netloc)\n","    return domain / url_len\n","  urldata['len_Domain_ratio'] = urldata['url'].apply(lambda i: len_Domain_ratio(i))\n","\n","#Path와 URL의 길이 비율\n","  def len_Path_ratio(url):\n","    url = urlparse(url)\n","    url_len = len(url)\n","    path = len(url.path)\n","    return path / url_len\n","  urldata['len_Path_ratio'] = urldata['url'].apply(lambda i: len_Path_ratio(i))\n","\n","#파라미터와 URL의 길이 비율\n","  def len_Params_ratio(url):\n","    url = urlparse(url)\n","    url_len = len(url)\n","    params = len(url.params)\n","    return params / url_len\n","  urldata['len_Params_ratio'] = urldata['url'].apply(lambda i: len_Params_ratio(i))\n","\n","#Query와 URL의 길이 비율\n","  def len_Query_ratio(url):\n","    url = urlparse(url)\n","    url_len = len(url)\n","    query = len(url.query)\n","    return query / url_len\n","  urldata['len_Query_ratio'] = urldata['url'].apply(lambda i: len_Query_ratio(i))\n","\n","#fragment와 URL의 길이 비율\n","  def len_Fragment_ratio(url):\n","    url = urlparse(url)\n","    url_len = len(url)\n","    fragment = len(url.fragment)\n","    return fragment / url_len\n","  urldata['len_Fragment_ratio'] = urldata['url'].apply(lambda i: len_Fragment_ratio(i))\n","\n","#의심 단어\n","  def suspicious_word(url):\n","    a = re.findall('confirm|account|secure|websc|login|signin|submit|update|logon|secure|wp|cmd|admin|ebayisapi', url)\n","    return len(a)\n","  urldata['suspicious_word'] = urldata['url'].apply(lambda i: suspicious_word(i))\n","\n","#Famous Domain check\n","  alexa_10k = pd.read_csv(colab_path + 'cloudflare-radar-domains-top-100000-20230821-20230828.csv')\n","  alexa_10k_list = []\n","\n","  for i in alexa_10k.index.values:\n","    alexa_10k_list.append(alexa_10k['domain'][i])\n","\n","  def dom_alexa_rank(url):\n","    parse = urlparse(url)\n","    domain = parse.netloc\n","    if domain in alexa_10k_list:\n","      return 1\n","    else:\n","      return -1\n","  urldata['dom_alexa_rank'] = urldata['url'].apply(lambda i: dom_alexa_rank(i))\n","\n","#각 URL의 엔트로피 계산\n","  def entropy(url):\n","    url = url.lower() # 알파벳 개수 세야 해서 소문자로 통일\n","    url_dict = {} # 알파벳 개수 중복 피해야 해서 일단 Dictionary 사용했음\n","    url_len = len(url) # url 길이\n","    p_i = pp_i = entropy = 0\n","  # 위 공식 참고 , pp_i는 (p_i * log2(p_i)) 를 의미함\n","    for i in url:\n","      url_dict[i] = url.count(i)\n","    url_dict = list(url_dict.values()) # 원할하게 하려고 리스트로 바꿨음\n","\n","    for j in url_dict:\n","      p_i = j / url_len\n","      pp_i = p_i * np.log2(p_i)\n","      entropy += pp_i\n","    return -(entropy)\n","  urldata['entropy'] = urldata['url'].apply(lambda i: entropy(i))\n","\n","  def protocol(url):\n","    if url[0:5] == 'https':\n","      return -1\n","    elif  url[0:4] == 'http':\n","      return 1\n","    return 0\n","  urldata['protocol'] = urldata['url'].apply(lambda i: protocol(i))\n","\n","#검색량\n","  #def search_url_amount(url):\n","    #Daum_url='https://search.daum.net/search?w=tot&DA=YZR&t_nil_searchbox=btn&sug=&sugo=&sg=&o=&q='\n","    #strOri='&sm=tab_org&qvt=0'\n","    #response = requests.get(Daum_url + url +strOri)\n","    #getlen=len(response.text)\n","    #return getlen\n","  #urldata['search_url_amount'] = urldata['url'].apply(lambda i: search_url_amount(i))\n","  # 검색량 부분을 추가하니 5시간 돌려도 안끝나서 일단 주석처리\n","\n","  return urldata"]}]}